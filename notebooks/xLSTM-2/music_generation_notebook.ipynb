{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xLSTM Music Generation - Clean Pipeline\n",
    "\n",
    "This notebook provides a clean, modular approach to generating music with your trained xLSTM model.\n",
    "\n",
    "## Key Fixes from Your Original Code:\n",
    "\n",
    "1. **Memory Issue Fixed**: The problem was `max_length` growing with each iteration\n",
    "   - **Wrong**: `max_length = len(output.split()) + chunk_size` (creates quadratic memory growth)\n",
    "   - **Right**: Use fixed `max_length` OR sliding window context\n",
    "\n",
    "2. **Context Length**: You can use larger context during inference than training\n",
    "   - Trained with 2048 â†’ Can infer with 4096 or more\n",
    "   - But memory grows with contextÂ² in mLSTM\n",
    "\n",
    "3. **Modular Design**: Clean separation of generation and conversion logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello0\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e20037/miniconda/envs/xlstm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/scratch1/e20-fyp-xlstm-music-generation/e20fyptemp1/fyp-musicgen/repos/helibrunna\")\n",
    "\n",
    "from xlstm_music_generation import MusicGenerator, MIDIConverter, generate_music\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "os.environ['TORCH_CUDA_ARCH_LIST'] = '8.0;8.6;8.9'\n",
    "os.environ['MAX_JOBS'] = '4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /scratch1/e20-fyp-xlstm-music-generation/e20fyptemp1/fyp-musicgen/repos/helibrunna/output/xlstm_lmd_512d_2048ctx_12b/run_20260126-0516\n",
      "\u001b[32m\u001b[1m   â–„â–ˆ    â–ˆâ–„       â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â–„â–ˆ        â–„â–ˆ  â–€â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„     â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ    â–ˆâ–„  â–ˆâ–ˆâ–ˆâ–„â–„â–„â–„   â–ˆâ–ˆâ–ˆâ–„â–„â–„â–„      â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ \n",
      "\u001b[32m\u001b[22m  â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ       â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–€â–€â–€â–ˆâ–ˆâ–„ â–ˆâ–ˆâ–ˆâ–€â–€â–€â–ˆâ–ˆâ–„   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ \n",
      "\u001b[32m\u001b[1m  â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ    â–ˆâ–€  â–ˆâ–ˆâ–ˆ       â–ˆâ–ˆâ–ˆâ–Œ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ \n",
      "\u001b[32m\u001b[22m â–„â–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–ˆâ–ˆâ–ˆâ–„â–„  â–„â–ˆâ–ˆâ–ˆâ–„â–„â–„     â–ˆâ–ˆâ–ˆ       â–ˆâ–ˆâ–ˆâ–Œ  â–„â–ˆâ–ˆâ–ˆâ–„â–„â–„â–ˆâ–ˆâ–€   â–„â–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–ˆâ–ˆâ–€ â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ \n",
      "\u001b[32m\u001b[1mâ–€â–€â–ˆâ–ˆâ–ˆâ–€â–€â–€â–€â–ˆâ–ˆâ–ˆâ–€  â–€â–€â–ˆâ–ˆâ–ˆâ–€â–€â–€     â–ˆâ–ˆâ–ˆ       â–ˆâ–ˆâ–ˆâ–Œ â–€â–€â–ˆâ–ˆâ–ˆâ–€â–€â–€â–ˆâ–ˆâ–„  â–€â–€â–ˆâ–ˆâ–ˆâ–€â–€â–€â–€â–€   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ â–€â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ \n",
      "\u001b[32m\u001b[22m  â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ    â–ˆâ–„  â–ˆâ–ˆâ–ˆ       â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–„ â–€â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ \n",
      "\u001b[32m\u001b[1m  â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ     â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–Œ    â–„ â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ   â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ \n",
      "\u001b[32m\u001b[22m  â–ˆâ–ˆâ–ˆ    â–ˆâ–€      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–ˆâ–ˆ â–ˆâ–€   â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–€    â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–€   â–€â–ˆ   â–ˆâ–€   â–€â–ˆ   â–ˆâ–€    â–ˆâ–ˆâ–ˆ    â–ˆâ–€  \n",
      "\u001b[32m\u001b[1m                            â–€                             â–ˆâ–ˆâ–ˆ    â–ˆâ–ˆâ–ˆ                                             \n",
      "\u001b[32m\u001b[22m\n",
      "\u001b[32m\u001b[1mBy Dr. Tristan Behrens\n",
      "\u001b[0m\n",
      "{\n",
      "    \"embedding_dim\": 512,\n",
      "    \"num_blocks\": 12,\n",
      "    \"context_length\": 16384,\n",
      "    \"slstm_at\": [\n",
      "        3,\n",
      "        6,\n",
      "        9\n",
      "    ],\n",
      "    \"mlstm_block\": {\n",
      "        \"mlstm\": {\n",
      "            \"conv1d_kernel_size\": 4,\n",
      "            \"qkv_proj_blocksize\": 4,\n",
      "            \"num_heads\": 4\n",
      "        }\n",
      "    },\n",
      "    \"slstm_block\": {\n",
      "        \"slstm\": {\n",
      "            \"backend\": \"cuda\",\n",
      "            \"num_heads\": 4,\n",
      "            \"conv1d_kernel_size\": 4,\n",
      "            \"bias_init\": \"powerlaw_blockdependent\"\n",
      "        },\n",
      "        \"feedforward\": {\n",
      "            \"proj_factor\": 1.3,\n",
      "            \"act_fn\": \"gelu\"\n",
      "        }\n",
      "    },\n",
      "    \"vocab_size\": 675\n",
      "}\n",
      "Creating xLSTMLMModel...\n",
      "{'verbose': True, 'with_cuda': True, 'extra_ldflags': ['-L/home/e20037/miniconda/envs/xlstm/lib', '-lcublas'], 'extra_cflags': ['-DSLSTM_HIDDEN_SIZE=512', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__'], 'extra_cuda_cflags': ['-Xptxas=\"-v\"', '-gencode', 'arch=compute_80,code=compute_80', '-res-usage', '--use_fast_math', '-O3', '-Xptxas -O3', '--extra-device-vectorization', '-DSLSTM_HIDDEN_SIZE=512', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/e20037/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/e20037/.cache/torch_extensions/py311_cu124/slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0/build.ninja...\n",
      "Building extension module slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n",
      "Using envvar MAX_JOBS (4) as the number of workers...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1] /home/e20037/miniconda/envs/xlstm/bin/x86_64-conda-linux-gnu-c++ slstm.o slstm_forward.cuda.o slstm_backward.cuda.o slstm_backward_cut.cuda.o slstm_pointwise.cuda.o blas.cuda.o cuda_error.cuda.o -shared -L/home/e20037/miniconda/envs/xlstm/lib -lcublas -L/home/e20037/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/home/e20037/miniconda/envs/xlstm/lib -lcudart -o slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n",
      "/home/e20037/miniconda/envs/xlstm/lib/python3.11/site-packages/xlstm/blocks/slstm/cell.py:543: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @conditional_decorator(\n",
      "/home/e20037/miniconda/envs/xlstm/lib/python3.11/site-packages/xlstm/blocks/slstm/cell.py:568: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @conditional_decorator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': True, 'with_cuda': True, 'extra_ldflags': ['-L/home/e20037/miniconda/envs/xlstm/lib', '-lcublas'], 'extra_cflags': ['-DSLSTM_HIDDEN_SIZE=512', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__'], 'extra_cuda_cflags': ['-Xptxas=\"-v\"', '-gencode', 'arch=compute_80,code=compute_80', '-res-usage', '--use_fast_math', '-O3', '-Xptxas -O3', '--extra-device-vectorization', '-DSLSTM_HIDDEN_SIZE=512', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/e20037/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0, skipping build step...\n",
      "Loading extension module slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': True, 'with_cuda': True, 'extra_ldflags': ['-L/home/e20037/miniconda/envs/xlstm/lib', '-lcublas'], 'extra_cflags': ['-DSLSTM_HIDDEN_SIZE=512', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__'], 'extra_cuda_cflags': ['-Xptxas=\"-v\"', '-gencode', 'arch=compute_80,code=compute_80', '-res-usage', '--use_fast_math', '-O3', '-Xptxas -O3', '--extra-device-vectorization', '-DSLSTM_HIDDEN_SIZE=512', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/e20037/.cache/torch_extensions/py311_cu124 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0, skipping build step...\n",
      "Loading extension module slstm_HS512BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n",
      "/home/e20037/miniconda/envs/xlstm/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2165: FutureWarning: Calling PreTrainedTokenizerFast.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Model loaded (context: 16384 tokens)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e20037/miniconda/envs/xlstm/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# MODEL_PATH = \"/scratch1/e20-fyp-xlstm-music-generation/e20fyptemp1/fyp-musicgen/repos/helibrunna/output/lmd_remigen_xlstm/run_20260115-1028\"\n",
    "\n",
    "# Context-2048, embedding-512\n",
    "MODEL_PATH = \"/scratch1/e20-fyp-xlstm-music-generation/e20fyptemp1/fyp-musicgen/repos/helibrunna/output/xlstm_lmd_512d_2048ctx_12b/run_20260126-0516\"\n",
    "\n",
    "# For short sequences (< 2048 tokens)\n",
    "generator = MusicGenerator(\n",
    "    model_path=MODEL_PATH,\n",
    "    context_length=16_384,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "converter = MIDIConverter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BATCH GENERATION\n",
      "======================================================================\n",
      "Temperatures: [0.6, 0.7, 0.8, 0.9]\n",
      "Target bars: [32, 48, 64]\n",
      "Pieces per combination: 5\n",
      "Total pieces: 60\n",
      "Output directory: ./generated_batch_20260128_191749\n",
      "======================================================================\n",
      "\n",
      "[1/60] Generating temp_0.6_bars_32_001.mid...\n",
      "âœ— Decoding error: AssertionError: \n",
      "   First attempt failed, trying with cleaning...\n",
      "   âœ“ Success - 24 bars, 4068 tokens\n",
      "   Time: 224.5s, Cleaning: Yes\n",
      "\n",
      "[2/60] Generating temp_0.6_bars_32_002.mid...\n",
      "âœ— Decoding error: AssertionError: \n",
      "   First attempt failed, trying with cleaning...\n",
      "   âœ“ Success - 32 bars, 5226 tokens\n",
      "   Time: 256.9s, Cleaning: Yes\n",
      "\n",
      "[3/60] Generating temp_0.6_bars_32_003.mid...\n"
     ]
    }
   ],
   "source": [
    "batch_results = generator.generate_batch(\n",
    "    temperatures=[0.6, 0.7, 0.8, 0.9],\n",
    "    target_bars_list=[32, 48, 64],\n",
    "    pieces_per_combination=5,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"s-9 o-42 t-36\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽµ Long generation (bar-aware chunking)...\n",
      "   Target: 10 bars\n",
      "   Strategy: Generate 2-3 bars per iteration, cut at b-1\n",
      "\n",
      "ðŸ“ Iteration 1/50\n",
      "   Context: 3 tokens (0 bars)\n",
      "   Generated: 400 tokens\n",
      "   Kept (complete bars): 227 tokens (3 bars)\n",
      "   Total: 230 tokens (3 bars)\n",
      "\n",
      "ðŸ“ Iteration 2/50\n",
      "   Context: 230 tokens (3 bars)\n",
      "   Generated: 400 tokens\n",
      "   Kept (complete bars): 356 tokens (3 bars)\n",
      "   Total: 586 tokens (6 bars)\n",
      "\n",
      "ðŸ“ Iteration 3/50\n",
      "   Context: 586 tokens (6 bars)\n",
      "   Generated: 400 tokens\n",
      "   Kept (complete bars): 362 tokens (3 bars)\n",
      "   Total: 948 tokens (9 bars)\n",
      "\n",
      "ðŸ“ Iteration 4/50\n",
      "   Context: 948 tokens (9 bars)\n",
      "   Generated: 400 tokens\n",
      "   Kept (complete bars): 238 tokens (2 bars)\n",
      "   Total: 1186 tokens (11 bars)\n",
      "\n",
      "âœ“ Reached target: 11 bars\n",
      "\n",
      "âœ“ Generation complete!\n",
      "   Final: 1186 tokens, 11 bars\n"
     ]
    }
   ],
   "source": [
    "result = generator.generate_long(\n",
    "    prompt=prompt,\n",
    "    temperature=0.8,\n",
    "    target_bars=10,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TOKEN ANALYSIS REPORT\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š OVERALL STATISTICS:\n",
      "   Total tokens: 1186\n",
      "   Total bars: 11\n",
      "   Total notes: 276\n",
      "   Unique instruments: 5\n",
      "\n",
      "ðŸŽµ BAR STATISTICS:\n",
      "   Average bar length: 106.8 tokens\n",
      "   Min bar length: 27 tokens\n",
      "   Max bar length: 123 tokens\n",
      "\n",
      "ðŸ”¤ TOKEN TYPES:\n",
      "   b-: 11\n",
      "   d-: 277\n",
      "   i-: 163\n",
      "   o-: 86\n",
      "   p-: 276\n",
      "   s-: 11\n",
      "   t-: 86\n",
      "   v-: 276\n",
      "\n",
      "ðŸŽ¹ INSTRUMENTS:\n",
      "   i-128\n",
      "   i-30\n",
      "   i-39\n",
      "   i-55\n",
      "   i-99\n",
      "\n",
      "âœ… SEQUENCE HEALTH:\n",
      "   Ends with b-1: âœ“\n",
      "   Grammar errors: 2\n",
      "\n",
      "âš ï¸  GRAMMAR ERRORS (showing first 5):\n",
      "   - Orphan token at 45: d-3\n",
      "   - Incomplete triplet at token 74: p-69 (missing duration)\n",
      "\n",
      "ðŸ” SEQUENCE EDGES:\n",
      "   First 10 tokens: s-9 o-42 t-36 i-55 p-64 d-3 v-30 p-57 d-3 v-30\n",
      "   Last 10 tokens: d-6 v-25 p-41 d-6 v-25 i-128 p-164 d-0 v-25 b-1\n",
      "============================================================\n",
      "Found 2 grammar errors!\n"
     ]
    }
   ],
   "source": [
    "# Analyze\n",
    "analysis = generator.analyze_tokens(result['tokens'], verbose=True)\n",
    "\n",
    "# Check for issues\n",
    "if analysis['has_errors']:\n",
    "    print(f\"Found {len(analysis['grammar_errors'])} grammar errors!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ— Decoding error: AssertionError: \n",
      "Success: False\n"
     ]
    }
   ],
   "source": [
    "# Try decoding WITHOUT cleaning\n",
    "success = converter.tokens_to_midi(result['tokens'], \"./output/long_clean.mid\", clean=False)\n",
    "print(f\"Success: {success}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: True\n"
     ]
    }
   ],
   "source": [
    "# Try decoding WITH cleaning\n",
    "success = converter.tokens_to_midi(result['tokens'], \"./output/long_clean.mid\", clean=True)\n",
    "print(f\"Success: {success}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug the last tokens\n",
    "tokens_list = result['tokens'].split()\n",
    "print(f\"Total: {len(tokens_list)}\")\n",
    "print(f\"Last 30 tokens: {tokens_list[-30:]}\")\n",
    "print(f\"Ends with b-1? {tokens_list[-1] == 'b-1'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽµ Generating...\n",
      "   Prompt: s-9 o-0 t-35...\n",
      "   Max tokens: 2000\n",
      "   Temperature: 0.8\n",
      "âœ“ Generated 2000 tokens (10 bars)\n"
     ]
    }
   ],
   "source": [
    "# Try single-shot generation with larger max_length\n",
    "result = generator.generate(\n",
    "    prompt=\"s-9 o-0 t-35\",\n",
    "    temperature=0.8,\n",
    "    max_tokens=2000,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Saved: ./output/ctx4096-full-trained/sample_1_ct4096-len1990.mid\n",
      "Generated 2000 tokens, 10 bars\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "output_path = \"./output/ctx2048-full-trained/sample_1_ct4096-len1990.mid\"\n",
    "success = converter.tokens_to_midi(result['tokens'], output_path, clean=False)\n",
    "\n",
    "if success:\n",
    "    print(f\"âœ“ Saved: {output_path}\")\n",
    "    print(f\"Generated {result['num_tokens']} tokens, {result['bars']} bars\")\n",
    "else:\n",
    "    print(\"Decoding failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: see what token is causing the error\n",
    "tokens = result['tokens'].split()\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"Last 30 tokens: {tokens[-30:]}\")\n",
    "\n",
    "# Find incomplete triplets\n",
    "for i, token in enumerate(tokens[-30:], start=len(tokens)-30):\n",
    "    if token.startswith('p-'):\n",
    "        if i+1 >= len(tokens) or not tokens[i+1].startswith('d-'):\n",
    "            print(f\"Incomplete at {i}: {token} (no duration)\")\n",
    "        elif i+2 >= len(tokens) or not tokens[i+2].startswith('v-'):\n",
    "            print(f\"Incomplete at {i}: {token} {tokens[i+1]} (no velocity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Generate Short Piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple generation\n",
    "result = generator.generate(\n",
    "    prompt=\"s-9 o-0 t-38\",\n",
    "    temperature=0.8,\n",
    "    max_tokens=3000,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {result['num_tokens']} tokens, {result['bars']} bars\")\n",
    "\n",
    "# Convert to MIDI\n",
    "output_path = \"./output/test_song.mid\"\n",
    "converter.tokens_to_midi(result['tokens'], output_path)\n",
    "print(f\"Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Generate Long Piece (Chunked)\n",
    "\n",
    "This uses **sliding window** approach to avoid memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For long generation, use larger context\n",
    "long_generator = MusicGenerator(\n",
    "    model_path=MODEL_PATH,\n",
    "    context_length=4096,  # Larger than training\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "result = long_generator.generate_long(\n",
    "    prompt=\"s-9 o-30 t-33 i-128 p-176 d-6 v-23 o-36 t-33 i-128 p-173 d-6 v-23 o-42 t-33 i-128 p-171 d-6 v-23 b-1 s-9 o-0 t-33 i-4 p-81 d-25\",\n",
    "    temperature=0.8,\n",
    "    target_bars=64,       # Generate 64 bars\n",
    "    chunk_tokens=1024,    # 1024 new tokens per iteration\n",
    "    max_iterations=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Save\n",
    "output_path = \"./output/long_song.mid\"\n",
    "converter.tokens_to_midi(result['tokens'], output_path)\n",
    "print(f\"\\nSaved {result['bars']} bars to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add detailed error reporting\n",
    "cleaned = converter.clean_tokens(result['tokens'])\n",
    "print(f\"Cleaned: {len(cleaned.split())} tokens\")\n",
    "print(f\"Last 20 cleaned: {cleaned.split()[-20:]}\")\n",
    "\n",
    "try:\n",
    "    midi_obj = converter.decoder.decode_from_token_str_list(cleaned.split())\n",
    "    print(\"âœ“ Decoding successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {type(e).__name__}: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Batch Generation with Different Temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = [0.5, 0.8, 1.0, 1.2]\n",
    "output_dir = Path(\"./output/temp_comparison\")\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Temperature: {temp}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    result = generator.generate(\n",
    "        prompt=\"s-9 o-0 t-35\",\n",
    "        temperature=temp,\n",
    "        max_tokens=2000,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    midi_path = output_dir / f\"temp_{temp:.1f}.mid\"\n",
    "    converter.tokens_to_midi(result['tokens'], str(midi_path))\n",
    "    print(f\"âœ“ Saved: {midi_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Simple API (One Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 5 songs with one function call\n",
    "outputs = generate_music(\n",
    "    model_path=MODEL_PATH,\n",
    "    num_songs=5,\n",
    "    max_tokens=2048,\n",
    "    temperature=0.8,\n",
    "    output_dir=\"./output/batch\"\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(outputs)} songs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Long Mode with Simple API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate long pieces\n",
    "outputs = generate_music(\n",
    "    model_path=MODEL_PATH,\n",
    "    num_songs=2,\n",
    "    temperature=0.8,\n",
    "    output_dir=\"./output/long_batch\",\n",
    "    long_mode=True,\n",
    "    target_bars=64\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(outputs)} long songs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Generated MIDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "\n",
    "def analyze_midi(midi_path):\n",
    "    midi = pretty_midi.PrettyMIDI(str(midi_path))\n",
    "    \n",
    "    print(f\"File: {midi_path.name}\")\n",
    "    print(f\"Duration: {midi.get_end_time():.2f}s\")\n",
    "    print(f\"Instruments: {len(midi.instruments)}\")\n",
    "    print(f\"Total notes: {sum(len(inst.notes) for inst in midi.instruments)}\")\n",
    "    \n",
    "    for inst in midi.instruments:\n",
    "        inst_type = \"Drums\" if inst.is_drum else f\"Program {inst.program}\"\n",
    "        print(f\"  - {inst.name}: {len(inst.notes)} notes ({inst_type})\")\n",
    "    print()\n",
    "\n",
    "# Analyze all generated files\n",
    "for midi_file in Path(\"./output\").glob(\"**/*.mid\"):\n",
    "    analyze_midi(midi_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Context Length\n",
    "\n",
    "### Training vs Inference:\n",
    "- **Training**: Model was trained with `context_length=2048`\n",
    "- **Inference**: You can use `context_length=4096` or higher\n",
    "  - The model can handle longer sequences\n",
    "  - But memory usage grows quadratically (NÂ² for mLSTM)\n",
    "\n",
    "### Memory Usage:\n",
    "- `context_length=2048` â†’ ~10GB VRAM\n",
    "- `context_length=4096` â†’ ~40GB VRAM  \n",
    "- `context_length=8192` â†’ ~160GB VRAM (likely OOM)\n",
    "\n",
    "### Solution for Long Generation:\n",
    "Use **sliding window** (implemented in `generate_long()`):\n",
    "- Keep only last N tokens as context\n",
    "- Generate new chunk\n",
    "- Slide window forward\n",
    "- Repeat\n",
    "\n",
    "This keeps memory constant while generating arbitrarily long sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Prompts\n",
    "\n",
    "REMIGEN format: `s-X o-Y t-Z i-A p-B d-C v-D ...`\n",
    "\n",
    "- `s-X`: Signature (time signature)\n",
    "- `o-Y`: Offset (timing)\n",
    "- `t-Z`: Tempo\n",
    "- `i-A`: Instrument\n",
    "- `p-B`: Pitch\n",
    "- `d-C`: Duration\n",
    "- `v-D`: Velocity\n",
    "- `b-1`: Bar marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different starting prompts\n",
    "prompts = [\n",
    "    \"s-9 o-0 t-35\",  # Slow tempo\n",
    "    \"s-9 o-0 t-120\", # Fast tempo\n",
    "    \"s-9 o-0 t-60 i-0\",  # With piano\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    result = generator.generate(\n",
    "        prompt=prompt,\n",
    "        temperature=0.8,\n",
    "        max_tokens=1500,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    midi_path = f\"./output/custom_prompt_{i}.mid\"\n",
    "    converter.tokens_to_midi(result['tokens'], midi_path)\n",
    "    print(f\"Saved: {midi_path}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Museformer\n",
    "\n",
    "For your research comparison, you can now:\n",
    "\n",
    "1. Generate same number of pieces from both models\n",
    "2. Use same prompts/seeds\n",
    "3. Compare:\n",
    "   - Musicality\n",
    "   - Coherence over long sequences\n",
    "   - Diversity\n",
    "   - Computational requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate evaluation dataset\n",
    "eval_output = Path(\"./evaluation/xlstm_samples\")\n",
    "eval_output.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for i in range(20):  # Generate 20 samples\n",
    "    result = generator.generate(\n",
    "        prompt=\"s-9 o-0 t-35\",\n",
    "        temperature=0.8,\n",
    "        max_tokens=2048,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    midi_path = eval_output / f\"xlstm_{i:03d}.mid\"\n",
    "    converter.tokens_to_midi(result['tokens'], str(midi_path))\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"Generated {i+1}/20 samples\")\n",
    "\n",
    "print(f\"\\nâœ“ Evaluation dataset ready: {eval_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
