{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7766ab91",
   "metadata": {},
   "source": [
    "# Music Generation with xLSTM + REMIGEN\n",
    "\n",
    "This notebook generates music using our trained xLSTM model and decodes\n",
    "REMIGEN tokens to MIDI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f1da43",
   "metadata": {},
   "source": [
    "Once the `xlstm` conda environment is set up, make sure to install the `midiProcessor`.\n",
    "\n",
    "```python\n",
    "cd ./repos/MidiProcessor\n",
    "pip install .\n",
    "pip install miditoolkit==0.1.16 numpy scipy pretty_midi mido tqdm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed4ec5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e20037/miniconda/envs/xlstm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/scratch1/e20-fyp-xlstm-music-generation/e20fyptemp1/fyp-musicgen/repos/helibrunna\")\n",
    "\n",
    "from source.languagemodel import LanguageModel\n",
    "import midiprocessor as mp\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"‚úì Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad751a",
   "metadata": {},
   "source": [
    "## Load xLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74626cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   ‚ñÑ‚ñà    ‚ñà‚ñÑ       ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñÑ‚ñà        ‚ñÑ‚ñà  ‚ñÄ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ     ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà    ‚ñà‚ñÑ  ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ   ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ      ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà \n",
      "\u001b[32m\u001b[22m  ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà     ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà       ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ‚ñà‚ñà‚ñÑ ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ‚ñà‚ñà‚ñÑ   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà \n",
      "\u001b[32m\u001b[1m  ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà     ‚ñà‚ñà‚ñà    ‚ñà‚ñÄ  ‚ñà‚ñà‚ñà       ‚ñà‚ñà‚ñà‚ñå   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà \n",
      "\u001b[32m\u001b[22m ‚ñÑ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ  ‚ñÑ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ     ‚ñà‚ñà‚ñà       ‚ñà‚ñà‚ñà‚ñå  ‚ñÑ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñÄ   ‚ñÑ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñÄ ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà \n",
      "\u001b[32m\u001b[1m‚ñÄ‚ñÄ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ‚ñÄ‚ñà‚ñà‚ñà‚ñÄ  ‚ñÄ‚ñÄ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ     ‚ñà‚ñà‚ñà       ‚ñà‚ñà‚ñà‚ñå ‚ñÄ‚ñÄ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ‚ñà‚ñà‚ñÑ  ‚ñÄ‚ñÄ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ‚ñÄ‚ñÄ   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà ‚ñÄ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà \n",
      "\u001b[32m\u001b[22m  ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà     ‚ñà‚ñà‚ñà    ‚ñà‚ñÑ  ‚ñà‚ñà‚ñà       ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñÑ ‚ñÄ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà \n",
      "\u001b[32m\u001b[1m  ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà     ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñå    ‚ñÑ ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà \n",
      "\u001b[32m\u001b[22m  ‚ñà‚ñà‚ñà    ‚ñà‚ñÄ      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñà‚ñà ‚ñà‚ñÄ   ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÄ    ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÄ   ‚ñÄ‚ñà   ‚ñà‚ñÄ   ‚ñÄ‚ñà   ‚ñà‚ñÄ    ‚ñà‚ñà‚ñà    ‚ñà‚ñÄ  \n",
      "\u001b[32m\u001b[1m                            ‚ñÄ                             ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà                                             \n",
      "\u001b[32m\u001b[22m\n",
      "\u001b[32m\u001b[1mBy Dr. Tristan Behrens\n",
      "\u001b[0m\n",
      "{\n",
      "    \"num_blocks\": 12,\n",
      "    \"embedding_dim\": 256,\n",
      "    \"mlstm_block\": {\n",
      "        \"mlstm\": {\n",
      "            \"num_heads\": 4,\n",
      "            \"conv1d_kernel_size\": 4\n",
      "        }\n",
      "    },\n",
      "    \"slstm_block\": {\n",
      "        \"slstm\": {\n",
      "            \"num_heads\": 4,\n",
      "            \"conv1d_kernel_size\": 4\n",
      "        }\n",
      "    },\n",
      "    \"slstm_at\": [\n",
      "        3,\n",
      "        6,\n",
      "        9\n",
      "    ],\n",
      "    \"context_length\": 16384,\n",
      "    \"vocab_size\": 675\n",
      "}\n",
      "Creating xLSTMLMModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xLSTMLMModel(\n",
      "  (xlstm_block_stack): xLSTMBlockStack(\n",
      "    (blocks): ModuleList(\n",
      "      (0-2): 3 x mLSTMBlock(\n",
      "        (xlstm_norm): LayerNorm()\n",
      "        (xlstm): mLSTMLayer(\n",
      "          (proj_up): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (q_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (k_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (v_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (conv1d): CausalConv1d(\n",
      "            (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)\n",
      "          )\n",
      "          (conv_act_fn): SiLU()\n",
      "          (mlstm_cell): mLSTMCell(\n",
      "            (igate): Linear(in_features=1536, out_features=4, bias=True)\n",
      "            (fgate): Linear(in_features=1536, out_features=4, bias=True)\n",
      "            (outnorm): MultiHeadLayerNorm()\n",
      "          )\n",
      "          (ogate_act_fn): SiLU()\n",
      "          (proj_down): Linear(in_features=512, out_features=256, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): sLSTMBlock(\n",
      "        (xlstm_norm): LayerNorm()\n",
      "        (xlstm): sLSTMLayer(\n",
      "          (conv1d): CausalConv1d(\n",
      "            (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)\n",
      "          )\n",
      "          (conv_act_fn): SiLU()\n",
      "          (fgate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (igate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (zgate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (ogate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=256, num_heads=4)\n",
      "          (group_norm): MultiHeadLayerNorm()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ffn_norm): LayerNorm()\n",
      "        (ffn): GatedFeedForward(\n",
      "          (proj_up): Linear(in_features=256, out_features=768, bias=False)\n",
      "          (proj_down): Linear(in_features=384, out_features=256, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4-5): 2 x mLSTMBlock(\n",
      "        (xlstm_norm): LayerNorm()\n",
      "        (xlstm): mLSTMLayer(\n",
      "          (proj_up): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (q_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (k_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (v_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (conv1d): CausalConv1d(\n",
      "            (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)\n",
      "          )\n",
      "          (conv_act_fn): SiLU()\n",
      "          (mlstm_cell): mLSTMCell(\n",
      "            (igate): Linear(in_features=1536, out_features=4, bias=True)\n",
      "            (fgate): Linear(in_features=1536, out_features=4, bias=True)\n",
      "            (outnorm): MultiHeadLayerNorm()\n",
      "          )\n",
      "          (ogate_act_fn): SiLU()\n",
      "          (proj_down): Linear(in_features=512, out_features=256, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): sLSTMBlock(\n",
      "        (xlstm_norm): LayerNorm()\n",
      "        (xlstm): sLSTMLayer(\n",
      "          (conv1d): CausalConv1d(\n",
      "            (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)\n",
      "          )\n",
      "          (conv_act_fn): SiLU()\n",
      "          (fgate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (igate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (zgate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (ogate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=256, num_heads=4)\n",
      "          (group_norm): MultiHeadLayerNorm()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ffn_norm): LayerNorm()\n",
      "        (ffn): GatedFeedForward(\n",
      "          (proj_up): Linear(in_features=256, out_features=768, bias=False)\n",
      "          (proj_down): Linear(in_features=384, out_features=256, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7-8): 2 x mLSTMBlock(\n",
      "        (xlstm_norm): LayerNorm()\n",
      "        (xlstm): mLSTMLayer(\n",
      "          (proj_up): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (q_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (k_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (v_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (conv1d): CausalConv1d(\n",
      "            (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)\n",
      "          )\n",
      "          (conv_act_fn): SiLU()\n",
      "          (mlstm_cell): mLSTMCell(\n",
      "            (igate): Linear(in_features=1536, out_features=4, bias=True)\n",
      "            (fgate): Linear(in_features=1536, out_features=4, bias=True)\n",
      "            (outnorm): MultiHeadLayerNorm()\n",
      "          )\n",
      "          (ogate_act_fn): SiLU()\n",
      "          (proj_down): Linear(in_features=512, out_features=256, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): sLSTMBlock(\n",
      "        (xlstm_norm): LayerNorm()\n",
      "        (xlstm): sLSTMLayer(\n",
      "          (conv1d): CausalConv1d(\n",
      "            (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)\n",
      "          )\n",
      "          (conv_act_fn): SiLU()\n",
      "          (fgate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (igate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (zgate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (ogate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=256, num_heads=4)\n",
      "          (group_norm): MultiHeadLayerNorm()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ffn_norm): LayerNorm()\n",
      "        (ffn): GatedFeedForward(\n",
      "          (proj_up): Linear(in_features=256, out_features=768, bias=False)\n",
      "          (proj_down): Linear(in_features=384, out_features=256, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10-11): 2 x mLSTMBlock(\n",
      "        (xlstm_norm): LayerNorm()\n",
      "        (xlstm): mLSTMLayer(\n",
      "          (proj_up): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (q_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (k_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (v_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (conv1d): CausalConv1d(\n",
      "            (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)\n",
      "          )\n",
      "          (conv_act_fn): SiLU()\n",
      "          (mlstm_cell): mLSTMCell(\n",
      "            (igate): Linear(in_features=1536, out_features=4, bias=True)\n",
      "            (fgate): Linear(in_features=1536, out_features=4, bias=True)\n",
      "            (outnorm): MultiHeadLayerNorm()\n",
      "          )\n",
      "          (ogate_act_fn): SiLU()\n",
      "          (proj_down): Linear(in_features=512, out_features=256, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_blocks_norm): LayerNorm()\n",
      "  )\n",
      "  (token_embedding): Embedding(675, 256)\n",
      "  (emb_dropout): Identity()\n",
      "  (lm_head): Linear(in_features=256, out_features=675, bias=False)\n",
      ")\n",
      "Number of parameters: 5_372_488\n",
      "Number of parameters: 5.37M\n",
      "Total size of the model: 20.49MB for precision 32-bit floats.\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/scratch1/e20-fyp-xlstm-music-generation/e20fyptemp1/fyp-musicgen/repos/helibrunna/output/lmd_remigen_xlstm/run_20260115-1028\"\n",
    "\n",
    "model = LanguageModel(\n",
    "    model_path,\n",
    "    config_overrides={\"context_length\": 16_384},  # Use full context\n",
    "    device=\"cuda\"  # or \"cpu\" if no GPU\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54f5d3f",
   "metadata": {},
   "source": [
    "## Generation Parameters\n",
    "\n",
    "Settings for controlling the music generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "435cc674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_prompt = \"s-9 o-0 t-38 i-35 p-62 d-2 v-22 o-6 t-38 i-35 p-62 d-2 v-17\"\n",
    "\n",
    "# Force specific instruments in the prompt\n",
    "# start_prompt = \"s-9 o-0 t-38 i-0 p-60 d-4 v-20 i-33 p-48 d-4 v-20 i-128 p-170 d-2 v-20\"\n",
    "#                        ^^^ Piano  ^^^ Bass       ^^^ Drums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b2985ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.8\n",
      "Max length: 5096 tokens\n",
      "Starting prompt: s-9 o-0 t-35 i-128 p-170 d-3 v-31 o-12 t-35 i-128 p-170 d-3 v-25 o-24 t-35 i-128 p-170 d-3 v-25 o-36 t-35 i-128 p-170 d-3 v-25 b-1 s-9 o-0 t-35 i-30 p-65 d-15 v-25 p-60 d-15 v-25 p-53 d-15 v-25 p-41\n",
      "Output directory: generated_music\n"
     ]
    }
   ],
   "source": [
    "# Temperature: controls randomness (0.5-1.5 recommended)\n",
    "# Lower = more predictable, Higher = more creative\n",
    "temperature = 0.8\n",
    "\n",
    "# Maximum length in tokens\n",
    "max_length = 5096  # Full context length\n",
    "\n",
    "# Number of songs to generate\n",
    "num_songs = 2\n",
    "\n",
    "# Starting prompt (standard REMIGEN opening)\n",
    "# This tells the model: \"Start a new song in 9/8 time at tempo 38\"\n",
    "# start_prompt = \"s-9 o-0 t-38\"\n",
    "start_prompt = \"s-9 o-0 t-35 i-128 p-170 d-3 v-31 o-12 t-35 i-128 p-170 d-3 v-25 o-24 t-35 i-128 p-170 d-3 v-25 o-36 t-35 i-128 p-170 d-3 v-25 b-1 s-9 o-0 t-35 i-30 p-65 d-15 v-25 p-60 d-15 v-25 p-53 d-15 v-25 p-41\"\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path(\"./generated_music\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Temperature: {temperature}\")\n",
    "print(f\"Max length: {max_length} tokens\")\n",
    "print(f\"Starting prompt: {start_prompt}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc0a074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c75c24e",
   "metadata": {},
   "source": [
    "## Token Generation Function\n",
    "\n",
    "Generates REMIGEN tokens from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e6c0719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_remigen_tokens(\n",
    "    model,\n",
    "    prompt=\"s-9 o-0 t-38\",\n",
    "    temperature=0.8,\n",
    "    max_length=2048,\n",
    "    stop_at_bars=None,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate REMIGEN tokens from the xLSTM model.\n",
    "    \n",
    "    Args:\n",
    "        model: The LanguageModel instance\n",
    "        prompt: Starting prompt (should start with s-X o-0 t-X)\n",
    "        temperature: Sampling temperature\n",
    "        max_length: Maximum number of tokens to generate\n",
    "        stop_at_bars: If set, stop after generating N bars (b-1 tokens)\n",
    "        verbose: Print progress information\n",
    "    \n",
    "    Returns:\n",
    "        (tokens_string, info_dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üéµ Starting generation...\")\n",
    "        print(f\"   Prompt: {prompt}\")\n",
    "        print(f\"   Max tokens: {max_length:,}\")\n",
    "        print(f\"   Temperature: {temperature}\")\n",
    "        if stop_at_bars:\n",
    "            print(f\"   Target bars: {stop_at_bars}\")\n",
    "        print()\n",
    "    \n",
    "    # Generate with the model\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"‚è≥ Generating tokens...\", end=\"\", flush=True)\n",
    "    \n",
    "    output_dict = model.generate(\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_length=max_length,\n",
    "        end_tokens=[],\n",
    "        forbidden_tokens=[\"[PAD]\", \"[EOS]\"],\n",
    "        return_structured_output=True\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\" Done! ({elapsed:.1f}s)\")\n",
    "    \n",
    "    # Extract generated tokens\n",
    "    tokens = output_dict[\"output\"]\n",
    "    token_list = tokens.split()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üìä Generated {len(token_list):,} raw tokens\")\n",
    "    \n",
    "    # FILTER OUT INVALID TOKENS\n",
    "    valid_tokens = []\n",
    "    invalid_count = 0\n",
    "    \n",
    "    for token in token_list:\n",
    "        # Only keep tokens with format: prefix-value\n",
    "        if '-' in token and not token.startswith('['):\n",
    "            valid_tokens.append(token)\n",
    "        else:\n",
    "            invalid_count += 1\n",
    "            if verbose and invalid_count <= 5:  # Show first 5\n",
    "                print(f\"‚ö†Ô∏è  Filtered invalid token: {token}\")\n",
    "    \n",
    "    if verbose and invalid_count > 5:\n",
    "        print(f\"‚ö†Ô∏è  Filtered {invalid_count - 5} more invalid tokens...\")\n",
    "    \n",
    "    # Count bars in valid tokens\n",
    "    bar_count = sum(1 for t in valid_tokens if t == \"b-1\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"‚úì Valid tokens: {len(valid_tokens):,}\")\n",
    "        print(f\"‚úì Bars generated: {bar_count}\")\n",
    "    \n",
    "    # Rejoin\n",
    "    tokens = \" \".join(valid_tokens)\n",
    "    \n",
    "    # Optional: truncate at bar limit\n",
    "    if stop_at_bars is not None:\n",
    "        if verbose:\n",
    "            print(f\"‚úÇÔ∏è  Truncating to {stop_at_bars} bars...\")\n",
    "        \n",
    "        truncated = []\n",
    "        bars_seen = 0\n",
    "        \n",
    "        for token in valid_tokens:\n",
    "            truncated.append(token)\n",
    "            if token == \"b-1\":\n",
    "                bars_seen += 1\n",
    "                if bars_seen >= stop_at_bars:\n",
    "                    break\n",
    "        \n",
    "        tokens = \" \".join(truncated)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"‚úì Truncated to {len(truncated):,} tokens ({bars_seen} bars)\")\n",
    "    \n",
    "    # Add extra info to output dict\n",
    "    output_dict.update({\n",
    "        \"valid_tokens\": len(valid_tokens),\n",
    "        \"invalid_tokens\": invalid_count,\n",
    "        \"bars\": bar_count,\n",
    "        \"elapsed_time\": elapsed\n",
    "    })\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"‚ö° Speed: {len(valid_tokens)/elapsed:.1f} tokens/sec\")\n",
    "        print()\n",
    "    \n",
    "    return tokens, output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2c19e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing generation...\n",
      "üéµ Starting generation...\n",
      "   Prompt: s-9 o-0 t-35 i-128 p-170 d-3 v-31 o-12 t-35 i-128 p-170 d-3 v-25 o-24 t-35 i-128 p-170 d-3 v-25 o-36 t-35 i-128 p-170 d-3 v-25 b-1 s-9 o-0 t-35 i-30 p-65 d-15 v-25 p-60 d-15 v-25 p-53 d-15 v-25 p-41\n",
      "   Max tokens: 200\n",
      "   Temperature: 0.8\n",
      "\n",
      "‚è≥ Generating tokens... Done! (7.6s)\n",
      "üìä Generated 200 raw tokens\n",
      "‚úì Valid tokens: 200\n",
      "‚úì Bars generated: 2\n",
      "‚ö° Speed: 26.4 tokens/sec\n",
      "\n",
      "Generated 200 tokens\n",
      "Speed: 21.15 tokens/sec\n",
      "\n",
      "First 100 chars: s-9 o-0 t-35 i-128 p-170 d-3 v-31 o-12 t-35 i-128 p-170 d-3 v-25 o-24 t-35 i-128 p-170 d-3 v-25 o-36\n"
     ]
    }
   ],
   "source": [
    "# Test generation\n",
    "print(\"Testing generation...\")\n",
    "test_tokens, test_info = generate_remigen_tokens(\n",
    "    model,\n",
    "    prompt=start_prompt,\n",
    "    temperature=temperature,\n",
    "    max_length=200,  # Short test\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(test_tokens.split())} tokens\")\n",
    "print(f\"Speed: {test_info['tokens_per_second']:.2f} tokens/sec\")\n",
    "print(f\"\\nFirst 100 chars: {test_tokens[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad73a32",
   "metadata": {},
   "source": [
    "## Decode Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7734474",
   "metadata": {},
   "source": [
    "REMIGEN ‚Üí MIDI Decoder\n",
    "\n",
    "Converts generated REMIGEN tokens to MIDI files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62e12a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_remigen_to_midi(token_string, output_path):\n",
    "    \"\"\"\n",
    "    Decode REMIGEN tokens to MIDI file.\n",
    "    \n",
    "    Args:\n",
    "        token_string: Space-separated REMIGEN tokens\n",
    "        output_path: Path to save .mid file\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Split tokens\n",
    "        tokens = token_string.strip().split()\n",
    "        \n",
    "        # Initialize decoder\n",
    "        decoder = mp.MidiDecoder('REMIGEN')\n",
    "        \n",
    "        # Decode to MIDI object\n",
    "        midi_obj = decoder.decode_from_token_str_list(tokens)\n",
    "        \n",
    "        # Save\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        midi_obj.dump(output_path)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error decoding: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea7912b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Test decode successful: generated_music/test_decode.mid\n"
     ]
    }
   ],
   "source": [
    "# Test decoding\n",
    "test_output_path = output_dir / \"test_decode.mid\"\n",
    "success = decode_remigen_to_midi(test_tokens, str(test_output_path))\n",
    "\n",
    "if success:\n",
    "    print(f\"‚úì Test decode successful: {test_output_path}\")\n",
    "else:\n",
    "    print(\"‚úó Test decode failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e303708",
   "metadata": {},
   "source": [
    "## Generate Multiple Songs\n",
    "Generate a batch of music samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Generating {num_songs} songs...\\n\")\n",
    "\n",
    "generated_songs = []\n",
    "\n",
    "for i in range(num_songs):\n",
    "    print(f\"Generating song {i+1}/{num_songs}...\")\n",
    "    \n",
    "    # Generate tokens\n",
    "    tokens, info = generate_remigen_tokens(\n",
    "        model,\n",
    "        prompt=start_prompt,\n",
    "        temperature=temperature,\n",
    "        max_length=max_length,\n",
    "        # stop_at_bars=32  # Generate 32 bars per song\n",
    "    )\n",
    "    \n",
    "    # Save info\n",
    "    song_data = {\n",
    "        \"id\": i,\n",
    "        \"tokens\": tokens,\n",
    "        \"num_tokens\": len(tokens.split()),\n",
    "        \"generation_time\": info[\"elapsed_time\"],\n",
    "        \"tokens_per_sec\": info[\"tokens_per_second\"]\n",
    "    }\n",
    "    generated_songs.append(song_data)\n",
    "    \n",
    "    print(f\"  Generated {song_data['num_tokens']} tokens in {song_data['generation_time']:.2f}s\")\n",
    "    print(f\"  Speed: {song_data['tokens_per_sec']:.2f} tokens/sec\\n\")\n",
    "\n",
    "print(f\"‚úì Generated {len(generated_songs)} songs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d0168",
   "metadata": {},
   "source": [
    "# Iterative Generation for Long Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e80ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_long_remigen_iterative(\n",
    "    model,\n",
    "    start_prompt=\"s-9 o-0 t-38\",\n",
    "    temperature=0.8,\n",
    "    chunk_size=1500,\n",
    "    max_iterations=10,\n",
    "    stop_at_bars=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate long REMIGEN sequences iteratively.\n",
    "    \"\"\"\n",
    "    \n",
    "    output = start_prompt\n",
    "    total_bars = 0\n",
    "    \n",
    "    print(f\"üéµ Starting iterative generation...\")\n",
    "    print(f\"   Chunk size: {chunk_size} tokens\")\n",
    "    print(f\"   Max iterations: {max_iterations}\\n\")\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"üìù Iteration {iteration + 1}/{max_iterations}\")\n",
    "        print(f\"   Current length: {len(output.split())} tokens\")\n",
    "        \n",
    "        # Generate continuation\n",
    "        chunk, info = generate_remigen_tokens(\n",
    "            model,\n",
    "            prompt=output,  # Use full context\n",
    "            temperature=temperature,\n",
    "            max_length=len(output.split()) + chunk_size,  # ‚Üê IMPORTANT: context + new tokens\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Extract only NEW tokens (after the prompt)\n",
    "        chunk_tokens = chunk.split()\n",
    "        output_tokens = output.split()\n",
    "        \n",
    "        # Find where new tokens start\n",
    "        if len(chunk_tokens) > len(output_tokens):\n",
    "            new_tokens = chunk_tokens[len(output_tokens):]  # Get only new tokens\n",
    "            output = output + \" \" + \" \".join(new_tokens)  # APPEND new tokens\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  No new tokens generated, stopping.\")\n",
    "            break\n",
    "        \n",
    "        # Count bars\n",
    "        bars_in_output = output.count(\"b-1\")\n",
    "        new_bars = bars_in_output - total_bars\n",
    "        total_bars = bars_in_output\n",
    "        \n",
    "        print(f\"   Added {len(new_tokens)} new tokens ({new_bars} new bars)\")\n",
    "        print(f\"   Total: {len(output.split())} tokens, {total_bars} bars\\n\")\n",
    "        \n",
    "        # Check stopping conditions\n",
    "        if stop_at_bars and total_bars >= stop_at_bars:\n",
    "            print(f\"‚úì Reached target of {stop_at_bars} bars!\")\n",
    "            break\n",
    "        \n",
    "        # Clear CUDA cache\n",
    "        import torch\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"‚úì Generation complete!\")\n",
    "    print(f\"   Final: {len(output.split())} tokens, {total_bars} bars\")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34f97d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating long sequence iteratively...\n",
      "\n",
      "üéµ Starting iterative generation...\n",
      "   Chunk size: 1500 tokens\n",
      "   Max iterations: 5\n",
      "\n",
      "üìù Iteration 1/5\n",
      "   Current length: 40 tokens\n",
      "   Added 1500 new tokens (12 new bars)\n",
      "   Total: 1540 tokens, 12 bars\n",
      "\n",
      "üìù Iteration 2/5\n",
      "   Current length: 1540 tokens\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 47.37 GiB of which 108.31 MiB is free. Process 869539 has 890.00 MiB memory in use. Process 3386576 has 27.32 GiB memory in use. Process 3652956 has 8.60 GiB memory in use. Process 3665233 has 2.03 GiB memory in use. Including non-PyTorch memory, this process has 8.41 GiB memory in use. Of the allocated memory 7.54 GiB is allocated by PyTorch, and 235.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Use it!\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerating long sequence iteratively...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m tokens = \u001b[43mgenerate_long_remigen_iterative\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Safe size per iteration\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 5 iterations x 1500 = ~7500 tokens\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstop_at_bars\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Or set number of bars\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úì Generated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tokens.split())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tokens!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mgenerate_long_remigen_iterative\u001b[39m\u001b[34m(model, start_prompt, temperature, chunk_size, max_iterations, stop_at_bars)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Current length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(output.split())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tokens\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Generate continuation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m chunk, info = \u001b[43mgenerate_remigen_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use full context\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ‚Üê IMPORTANT: context + new tokens\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Extract only NEW tokens (after the prompt)\u001b[39;00m\n\u001b[32m     34\u001b[39m chunk_tokens = chunk.split()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mgenerate_remigen_tokens\u001b[39m\u001b[34m(model, prompt, temperature, max_length, stop_at_bars, verbose)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚è≥ Generating tokens...\u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m output_dict = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforbidden_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m[PAD]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m[EOS]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_structured_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     47\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m elapsed = time.time() - start_time\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch1/e20-fyp-xlstm-music-generation/e20fyptemp1/fyp-musicgen/repos/helibrunna/source/languagemodel.py:222\u001b[39m, in \u001b[36mLanguageModel.generate\u001b[39m\u001b[34m(self, prompt, temperature, max_length, end_tokens, forbidden_tokens, return_structured_output)\u001b[39m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;66;03m# Generate the continuation.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m outputs.shape[\u001b[32m0\u001b[39m] == \u001b[32m1\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Mask the tokens.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/xlstm/xlstm_lm_model.py:52\u001b[39m, in \u001b[36mxLSTMLMModel.forward\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     50\u001b[39m x = \u001b[38;5;28mself\u001b[39m.token_embedding(idx)\n\u001b[32m     51\u001b[39m x = \u001b[38;5;28mself\u001b[39m.emb_dropout(x)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mxlstm_block_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.lm_head(x)\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/xlstm/xlstm_block_stack.py:120\u001b[39m, in \u001b[36mxLSTMBlockStack.forward\u001b[39m\u001b[34m(self, x, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor, **kwargs) -> torch.Tensor:\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         x = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.post_blocks_norm(x)\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/xlstm/blocks/xlstm_block.py:77\u001b[39m, in \u001b[36mxLSTMBlock.forward\u001b[39m\u001b[34m(self, x, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor, **kwargs) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mxlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mxlstm_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ffn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     79\u001b[39m         x = x + \u001b[38;5;28mself\u001b[39m.ffn(\u001b[38;5;28mself\u001b[39m.ffn_norm(x), **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/xlstm/blocks/mlstm/layer.py:116\u001b[39m, in \u001b[36mmLSTMLayer.forward\u001b[39m\u001b[34m(self, x, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m k = \u001b[38;5;28mself\u001b[39m.k_proj(x_mlstm_conv_act)\n\u001b[32m    114\u001b[39m v = \u001b[38;5;28mself\u001b[39m.v_proj(x_mlstm)\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m h_tilde_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlstm_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m h_tilde_state_skip = h_tilde_state + (\u001b[38;5;28mself\u001b[39m.learnable_skip * x_mlstm_conv_act)\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# output / z branch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/xlstm/blocks/mlstm/cell.py:61\u001b[39m, in \u001b[36mmLSTMCell.forward\u001b[39m\u001b[34m(self, q, k, v, **kwargs)\u001b[39m\n\u001b[32m     58\u001b[39m fgate_preact = \u001b[38;5;28mself\u001b[39m.fgate(if_gate_input)  \u001b[38;5;66;03m# (B, S, NH)\u001b[39;00m\n\u001b[32m     59\u001b[39m fgate_preact = fgate_preact.transpose(-\u001b[32m1\u001b[39m, -\u001b[32m2\u001b[39m).unsqueeze(-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (B, NH, S, 1)#\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m h_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m=\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43migate_preact\u001b[49m\u001b[43m=\u001b[49m\u001b[43migate_preact\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfgate_preact\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfgate_preact\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlower_triangular_matrix\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, NH, S, DH)\u001b[39;00m\n\u001b[32m     70\u001b[39m h_state_norm = \u001b[38;5;28mself\u001b[39m.outnorm(h_state)  \u001b[38;5;66;03m# (B, NH, S, DH)\u001b[39;00m\n\u001b[32m     71\u001b[39m h_state_norm = h_state_norm.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).reshape(B, S, -\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (B, NH, S, DH) -> (B, S, NH, DH) -> (B, S, H)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/xlstm/blocks/mlstm/backends.py:85\u001b[39m, in \u001b[36mparallel_stabilized_simple\u001b[39m\u001b[34m(queries, keys, values, igate_preact, fgate_preact, lower_triangular_matrix, stabilize_rowwise, eps, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m normalizer = torch.maximum(C_matrix.sum(dim=-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m).abs(), torch.exp(-max_log_D))  \u001b[38;5;66;03m# (B, NH, S, 1)\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# (B, NH, S, S)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m C_matrix_normalized = \u001b[43mC_matrix\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# retrieved values\u001b[39;00m\n\u001b[32m     88\u001b[39m h_tilde_state = C_matrix_normalized @ values  \u001b[38;5;66;03m# (B, NH, S, DH)\u001b[39;00m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 114.00 MiB. GPU 0 has a total capacity of 47.37 GiB of which 108.31 MiB is free. Process 869539 has 890.00 MiB memory in use. Process 3386576 has 27.32 GiB memory in use. Process 3652956 has 8.60 GiB memory in use. Process 3665233 has 2.03 GiB memory in use. Including non-PyTorch memory, this process has 8.41 GiB memory in use. Of the allocated memory 7.54 GiB is allocated by PyTorch, and 235.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Use it!\n",
    "print(\"Generating long sequence iteratively...\\n\")\n",
    "\n",
    "tokens = generate_long_remigen_iterative(\n",
    "    model,\n",
    "    start_prompt=start_prompt,\n",
    "    temperature=0.8,\n",
    "    chunk_size=1500,  # Safe size per iteration\n",
    "    max_iterations=5,  # 5 iterations x 1500 = ~7500 tokens\n",
    "    stop_at_bars=64  # Or set number of bars\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Generated {len(tokens.split())} tokens!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78911664",
   "metadata": {},
   "source": [
    "### Decode All Generated Songs to MIDI\n",
    "\n",
    "Convert all generated token sequences to MIDI files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54597ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding to MIDI files...\n",
      "\n",
      "‚úì Song 0: generated_song_000.mid\n",
      "‚úì Song 1: generated_song_001.mid\n",
      "\n",
      "‚úì Successfully decoded: 2/2\n"
     ]
    }
   ],
   "source": [
    "print(\"Decoding to MIDI files...\\n\")\n",
    "\n",
    "successful = 0\n",
    "failed = 0\n",
    "\n",
    "for song_data in generated_songs:\n",
    "    song_id = song_data[\"id\"]\n",
    "    tokens = song_data[\"tokens\"]\n",
    "    \n",
    "    # Output path\n",
    "    midi_path = output_dir / f\"generated_song_{song_id:03d}.mid\"\n",
    "    \n",
    "    # Decode\n",
    "    success = decode_remigen_to_midi(tokens, str(midi_path))\n",
    "    \n",
    "    if success:\n",
    "        successful += 1\n",
    "        print(f\"‚úì Song {song_id}: {midi_path.name}\")\n",
    "    else:\n",
    "        failed += 1\n",
    "        print(f\"‚úó Song {song_id}: Failed to decode\")\n",
    "\n",
    "print(f\"\\n‚úì Successfully decoded: {successful}/{len(generated_songs)}\")\n",
    "if failed > 0:\n",
    "    print(f\"‚úó Failed: {failed}/{len(generated_songs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60502a7b",
   "metadata": {},
   "source": [
    "## Analyze Generated MIDI Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255fbe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing generated music...\n",
      "\n",
      "Song 0:\n",
      "  Duration: 17.75s\n",
      "  Instruments: 5\n",
      "  Total notes: 555\n",
      "    28: 62 notes (Program 28)\n",
      "    29: 261 notes (Program 29)\n",
      "    30: 145 notes (Program 30)\n",
      "    35: 38 notes (Program 35)\n",
      "    128: 49 notes (Drums)\n",
      "\n",
      "Song 1:\n",
      "  Duration: 24.85s\n",
      "  Instruments: 3\n",
      "  Total notes: 490\n",
      "    30: 218 notes (Program 30)\n",
      "    33: 95 notes (Program 33)\n",
      "    128: 177 notes (Drums)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pretty_midi\n",
    "\n",
    "print(\"Analyzing generated music...\\n\")\n",
    "\n",
    "for song_data in generated_songs[:3]:  # Analyze first 3 songs\n",
    "    song_id = song_data[\"id\"]\n",
    "    midi_path = output_dir / f\"generated_song_{song_id:03d}.mid\"\n",
    "    \n",
    "    if not midi_path.exists():\n",
    "        continue\n",
    "    \n",
    "    # Load MIDI\n",
    "    midi = pretty_midi.PrettyMIDI(str(midi_path))\n",
    "    \n",
    "    print(f\"Song {song_id}:\")\n",
    "    print(f\"  Duration: {midi.get_end_time():.2f}s\")\n",
    "    print(f\"  Instruments: {len(midi.instruments)}\")\n",
    "    print(f\"  Total notes: {sum(len(inst.notes) for inst in midi.instruments)}\")\n",
    "    \n",
    "    # Show instruments\n",
    "    for inst in midi.instruments:\n",
    "        inst_type = \"Drums\" if inst.is_drum else f\"Program {inst.program}\"\n",
    "        print(f\"    {inst.name}: {len(inst.notes)} notes ({inst_type})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5600537",
   "metadata": {},
   "source": [
    "## Save Generated Tokens\n",
    "\n",
    "Save raw token sequences for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab816eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved token files to generated_music/tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokens_dir = output_dir / \"tokens\"\n",
    "tokens_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for song_data in generated_songs:\n",
    "    song_id = song_data[\"id\"]\n",
    "    tokens = song_data[\"tokens\"]\n",
    "    \n",
    "    token_path = tokens_dir / f\"generated_song_{song_id:03d}.txt\"\n",
    "    \n",
    "    with open(token_path, 'w') as f:\n",
    "        f.write(tokens)\n",
    "\n",
    "print(f\"‚úì Saved token files to {tokens_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f64b36",
   "metadata": {},
   "source": [
    "## Generation Summary\n",
    "\n",
    "Summary of the music generation session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f62c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MUSIC GENERATION SUMMARY\n",
      "============================================================\n",
      "Model: /scratch1/e20-fyp-xlstm-music-generation/e20fyptemp1/fyp-musicgen/repos/helibrunna/output/lmd_remigen_xlstm/run_20260115-1028\n",
      "Songs generated: 2\n",
      "Temperature: 0.8\n",
      "Max tokens: 2048\n",
      "Output directory: generated_music\n",
      "\n",
      "Generated files:\n",
      "  MIDI files: generated_music/*.mid\n",
      "  Token files: generated_music/tokens/*.txt\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MUSIC GENERATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {model_path}\")\n",
    "print(f\"Songs generated: {len(generated_songs)}\")\n",
    "print(f\"Temperature: {temperature}\")\n",
    "print(f\"Max tokens: {max_length}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  MIDI files: {output_dir}/*.mid\")\n",
    "print(f\"  Token files: {tokens_dir}/*.txt\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d5185",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
