{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7766ab91",
   "metadata": {},
   "source": [
    "# Music Generation with xLSTM + REMIGEN\n",
    "\n",
    "This notebook generates music using our trained xLSTM model and decodes\n",
    "REMIGEN tokens to MIDI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f1da43",
   "metadata": {},
   "source": [
    "Once the `xlstm` conda environment is set up, make sure to install the `midiProcessor`.\n",
    "\n",
    "```python\n",
    "cd ./repos/MidiProcessor\n",
    "pip install .\n",
    "pip install miditoolkit==0.1.16 numpy scipy pretty_midi mido tqdm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ed4ec5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Imports successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/scratch1/e20-fyp-xlstm-music-generation/e20fyptemp1/fyp-musicgen/repos/helibrunna\")\n",
    "\n",
    "from source.languagemodel import LanguageModel\n",
    "import midiprocessor as mp\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"‚úì Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bad751a",
   "metadata": {},
   "source": [
    "## Load xLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "74626cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   ‚ñÑ‚ñà    ‚ñà‚ñÑ       ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚ñÑ‚ñà        ‚ñÑ‚ñà  ‚ñÄ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ     ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà    ‚ñà‚ñÑ  ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ   ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ      ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà \n",
      "\u001b[32m\u001b[22m  ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà     ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà       ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ‚ñà‚ñà‚ñÑ ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ‚ñà‚ñà‚ñÑ   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà \n",
      "\u001b[32m\u001b[1m  ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà     ‚ñà‚ñà‚ñà    ‚ñà‚ñÄ  ‚ñà‚ñà‚ñà       ‚ñà‚ñà‚ñà‚ñå   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà \n",
      "\u001b[32m\u001b[22m ‚ñÑ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ  ‚ñÑ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ     ‚ñà‚ñà‚ñà       ‚ñà‚ñà‚ñà‚ñå  ‚ñÑ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñÄ   ‚ñÑ‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñÄ ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà \n",
      "\u001b[32m\u001b[1m‚ñÄ‚ñÄ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ‚ñÄ‚ñà‚ñà‚ñà‚ñÄ  ‚ñÄ‚ñÄ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ     ‚ñà‚ñà‚ñà       ‚ñà‚ñà‚ñà‚ñå ‚ñÄ‚ñÄ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ‚ñà‚ñà‚ñÑ  ‚ñÄ‚ñÄ‚ñà‚ñà‚ñà‚ñÄ‚ñÄ‚ñÄ‚ñÄ‚ñÄ   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà ‚ñÄ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà \n",
      "\u001b[32m\u001b[22m  ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà     ‚ñà‚ñà‚ñà    ‚ñà‚ñÑ  ‚ñà‚ñà‚ñà       ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñÑ ‚ñÄ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà \n",
      "\u001b[32m\u001b[1m  ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà     ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñå    ‚ñÑ ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà   ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà \n",
      "\u001b[32m\u001b[22m  ‚ñà‚ñà‚ñà    ‚ñà‚ñÄ      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ‚ñÑ‚ñà‚ñà ‚ñà‚ñÄ   ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÄ    ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÄ   ‚ñÄ‚ñà   ‚ñà‚ñÄ   ‚ñÄ‚ñà   ‚ñà‚ñÄ    ‚ñà‚ñà‚ñà    ‚ñà‚ñÄ  \n",
      "\u001b[32m\u001b[1m                            ‚ñÄ                             ‚ñà‚ñà‚ñà    ‚ñà‚ñà‚ñà                                             \n",
      "\u001b[32m\u001b[22m\n",
      "\u001b[32m\u001b[1mBy Dr. Tristan Behrens\n",
      "\u001b[0m\n",
      "{\n",
      "    \"num_blocks\": 12,\n",
      "    \"embedding_dim\": 256,\n",
      "    \"mlstm_block\": {\n",
      "        \"mlstm\": {\n",
      "            \"num_heads\": 4,\n",
      "            \"conv1d_kernel_size\": 4\n",
      "        }\n",
      "    },\n",
      "    \"slstm_block\": {\n",
      "        \"slstm\": {\n",
      "            \"num_heads\": 4,\n",
      "            \"conv1d_kernel_size\": 4\n",
      "        }\n",
      "    },\n",
      "    \"slstm_at\": [\n",
      "        3,\n",
      "        6,\n",
      "        9\n",
      "    ],\n",
      "    \"context_length\": 8192,\n",
      "    \"vocab_size\": 671\n",
      "}\n",
      "Creating xLSTMLMModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e20037/miniconda/envs/xlstm/lib/python3.11/site-packages/xlstm/blocks/slstm/cell.py:543: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @conditional_decorator(\n",
      "/home/e20037/miniconda/envs/xlstm/lib/python3.11/site-packages/xlstm/blocks/slstm/cell.py:568: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @conditional_decorator(\n",
      "/home/e20037/miniconda/envs/xlstm/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2165: FutureWarning: Calling PreTrainedTokenizerFast.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xLSTMLMModel(\n",
      "  (xlstm_block_stack): xLSTMBlockStack(\n",
      "    (blocks): ModuleList(\n",
      "      (0-2): 3 x mLSTMBlock(\n",
      "        (xlstm_norm): LayerNorm()\n",
      "        (xlstm): mLSTMLayer(\n",
      "          (proj_up): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (q_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (k_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (v_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (conv1d): CausalConv1d(\n",
      "            (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)\n",
      "          )\n",
      "          (conv_act_fn): SiLU()\n",
      "          (mlstm_cell): mLSTMCell(\n",
      "            (igate): Linear(in_features=1536, out_features=4, bias=True)\n",
      "            (fgate): Linear(in_features=1536, out_features=4, bias=True)\n",
      "            (outnorm): MultiHeadLayerNorm()\n",
      "          )\n",
      "          (ogate_act_fn): SiLU()\n",
      "          (proj_down): Linear(in_features=512, out_features=256, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): sLSTMBlock(\n",
      "        (xlstm_norm): LayerNorm()\n",
      "        (xlstm): sLSTMLayer(\n",
      "          (conv1d): CausalConv1d(\n",
      "            (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)\n",
      "          )\n",
      "          (conv_act_fn): SiLU()\n",
      "          (fgate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (igate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (zgate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (ogate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=256, num_heads=4)\n",
      "          (group_norm): MultiHeadLayerNorm()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ffn_norm): LayerNorm()\n",
      "        (ffn): GatedFeedForward(\n",
      "          (proj_up): Linear(in_features=256, out_features=768, bias=False)\n",
      "          (proj_down): Linear(in_features=384, out_features=256, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4-5): 2 x mLSTMBlock(\n",
      "        (xlstm_norm): LayerNorm()\n",
      "        (xlstm): mLSTMLayer(\n",
      "          (proj_up): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (q_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (k_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (v_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (conv1d): CausalConv1d(\n",
      "            (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)\n",
      "          )\n",
      "          (conv_act_fn): SiLU()\n",
      "          (mlstm_cell): mLSTMCell(\n",
      "            (igate): Linear(in_features=1536, out_features=4, bias=True)\n",
      "            (fgate): Linear(in_features=1536, out_features=4, bias=True)\n",
      "            (outnorm): MultiHeadLayerNorm()\n",
      "          )\n",
      "          (ogate_act_fn): SiLU()\n",
      "          (proj_down): Linear(in_features=512, out_features=256, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): sLSTMBlock(\n",
      "        (xlstm_norm): LayerNorm()\n",
      "        (xlstm): sLSTMLayer(\n",
      "          (conv1d): CausalConv1d(\n",
      "            (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)\n",
      "          )\n",
      "          (conv_act_fn): SiLU()\n",
      "          (fgate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (igate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (zgate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (ogate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=256, num_heads=4)\n",
      "          (group_norm): MultiHeadLayerNorm()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ffn_norm): LayerNorm()\n",
      "        (ffn): GatedFeedForward(\n",
      "          (proj_up): Linear(in_features=256, out_features=768, bias=False)\n",
      "          (proj_down): Linear(in_features=384, out_features=256, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7-8): 2 x mLSTMBlock(\n",
      "        (xlstm_norm): LayerNorm()\n",
      "        (xlstm): mLSTMLayer(\n",
      "          (proj_up): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (q_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (k_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (v_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (conv1d): CausalConv1d(\n",
      "            (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)\n",
      "          )\n",
      "          (conv_act_fn): SiLU()\n",
      "          (mlstm_cell): mLSTMCell(\n",
      "            (igate): Linear(in_features=1536, out_features=4, bias=True)\n",
      "            (fgate): Linear(in_features=1536, out_features=4, bias=True)\n",
      "            (outnorm): MultiHeadLayerNorm()\n",
      "          )\n",
      "          (ogate_act_fn): SiLU()\n",
      "          (proj_down): Linear(in_features=512, out_features=256, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): sLSTMBlock(\n",
      "        (xlstm_norm): LayerNorm()\n",
      "        (xlstm): sLSTMLayer(\n",
      "          (conv1d): CausalConv1d(\n",
      "            (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)\n",
      "          )\n",
      "          (conv_act_fn): SiLU()\n",
      "          (fgate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (igate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (zgate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (ogate): LinearHeadwiseExpand(in_features=256, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=256, num_heads=4)\n",
      "          (group_norm): MultiHeadLayerNorm()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ffn_norm): LayerNorm()\n",
      "        (ffn): GatedFeedForward(\n",
      "          (proj_up): Linear(in_features=256, out_features=768, bias=False)\n",
      "          (proj_down): Linear(in_features=384, out_features=256, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10-11): 2 x mLSTMBlock(\n",
      "        (xlstm_norm): LayerNorm()\n",
      "        (xlstm): mLSTMLayer(\n",
      "          (proj_up): Linear(in_features=256, out_features=1024, bias=False)\n",
      "          (q_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (k_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (v_proj): LinearHeadwiseExpand(in_features=512, num_heads=128, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
      "          (conv1d): CausalConv1d(\n",
      "            (conv): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)\n",
      "          )\n",
      "          (conv_act_fn): SiLU()\n",
      "          (mlstm_cell): mLSTMCell(\n",
      "            (igate): Linear(in_features=1536, out_features=4, bias=True)\n",
      "            (fgate): Linear(in_features=1536, out_features=4, bias=True)\n",
      "            (outnorm): MultiHeadLayerNorm()\n",
      "          )\n",
      "          (ogate_act_fn): SiLU()\n",
      "          (proj_down): Linear(in_features=512, out_features=256, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_blocks_norm): LayerNorm()\n",
      "  )\n",
      "  (token_embedding): Embedding(671, 256)\n",
      "  (emb_dropout): Identity()\n",
      "  (lm_head): Linear(in_features=256, out_features=671, bias=False)\n",
      ")\n",
      "Number of parameters: 5_370_440\n",
      "Number of parameters: 5.37M\n",
      "Total size of the model: 20.49MB for precision 32-bit floats.\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e20037/miniconda/envs/xlstm/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/scratch1/e20-fyp-xlstm-music-generation/e20fyptemp1/fyp-musicgen/repos/helibrunna/output/lmd_remigen_xlstm/run_20260106-1858\"\n",
    "\n",
    "model = LanguageModel(\n",
    "    model_path,\n",
    "    config_overrides={\"context_length\": 8192},  # Use full context\n",
    "    device=\"cuda\"  # or \"cpu\" if no GPU\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54f5d3f",
   "metadata": {},
   "source": [
    "## Generation Parameters\n",
    "\n",
    "Settings for controlling the music generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "435cc674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_prompt = \"s-9 o-0 t-38 i-35 p-62 d-2 v-22 o-6 t-38 i-35 p-62 d-2 v-17\"\n",
    "\n",
    "# Force specific instruments in the prompt\n",
    "start_prompt = \"s-9 o-0 t-38 i-0 p-60 d-4 v-20 i-33 p-48 d-4 v-20 i-128 p-170 d-2 v-20\"\n",
    "#                        ^^^ Piano  ^^^ Bass       ^^^ Drums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2985ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.7\n",
      "Max length: 8000 tokens\n",
      "Starting prompt: s-9 o-0 t-38 i-0 p-60 d-4 v-20 i-33 p-48 d-4 v-20 i-128 p-170 d-2 v-20\n",
      "Output directory: generated_music\n"
     ]
    }
   ],
   "source": [
    "# Temperature: controls randomness (0.5-1.5 recommended)\n",
    "# Lower = more predictable, Higher = more creative\n",
    "temperature = 0.7\n",
    "\n",
    "# Maximum length in tokens\n",
    "max_length = 8000  # Full context length\n",
    "\n",
    "# Number of songs to generate\n",
    "num_songs = 1\n",
    "\n",
    "# Starting prompt (standard REMIGEN opening)\n",
    "# This tells the model: \"Start a new song in 9/8 time at tempo 38\"\n",
    "# start_prompt = \"s-9 o-0 t-38\"\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path(\"./generated_music\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Temperature: {temperature}\")\n",
    "print(f\"Max length: {max_length} tokens\")\n",
    "print(f\"Starting prompt: {start_prompt}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc0a074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c75c24e",
   "metadata": {},
   "source": [
    "## Token Generation Function\n",
    "\n",
    "Generates REMIGEN tokens from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4e6c0719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_remigen_tokens(\n",
    "    model,\n",
    "    prompt=\"s-9 o-0 t-38\",\n",
    "    temperature=0.8,\n",
    "    max_length=2048,\n",
    "    stop_at_bars=None,\n",
    "    verbose=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate REMIGEN tokens from the xLSTM model.\n",
    "    \n",
    "    Args:\n",
    "        model: The LanguageModel instance\n",
    "        prompt: Starting prompt (should start with s-X o-0 t-X)\n",
    "        temperature: Sampling temperature\n",
    "        max_length: Maximum number of tokens to generate\n",
    "        stop_at_bars: If set, stop after generating N bars (b-1 tokens)\n",
    "        verbose: Print progress information\n",
    "    \n",
    "    Returns:\n",
    "        (tokens_string, info_dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üéµ Starting generation...\")\n",
    "        print(f\"   Prompt: {prompt}\")\n",
    "        print(f\"   Max tokens: {max_length:,}\")\n",
    "        print(f\"   Temperature: {temperature}\")\n",
    "        if stop_at_bars:\n",
    "            print(f\"   Target bars: {stop_at_bars}\")\n",
    "        print()\n",
    "    \n",
    "    # Generate with the model\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"‚è≥ Generating tokens...\", end=\"\", flush=True)\n",
    "    \n",
    "    output_dict = model.generate(\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_length=max_length,\n",
    "        end_tokens=[],\n",
    "        forbidden_tokens=[\"[PAD]\", \"[EOS]\"],\n",
    "        return_structured_output=True\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\" Done! ({elapsed:.1f}s)\")\n",
    "    \n",
    "    # Extract generated tokens\n",
    "    tokens = output_dict[\"output\"]\n",
    "    token_list = tokens.split()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"üìä Generated {len(token_list):,} raw tokens\")\n",
    "    \n",
    "    # FILTER OUT INVALID TOKENS\n",
    "    valid_tokens = []\n",
    "    invalid_count = 0\n",
    "    \n",
    "    for token in token_list:\n",
    "        # Only keep tokens with format: prefix-value\n",
    "        if '-' in token and not token.startswith('['):\n",
    "            valid_tokens.append(token)\n",
    "        else:\n",
    "            invalid_count += 1\n",
    "            if verbose and invalid_count <= 5:  # Show first 5\n",
    "                print(f\"‚ö†Ô∏è  Filtered invalid token: {token}\")\n",
    "    \n",
    "    if verbose and invalid_count > 5:\n",
    "        print(f\"‚ö†Ô∏è  Filtered {invalid_count - 5} more invalid tokens...\")\n",
    "    \n",
    "    # Count bars in valid tokens\n",
    "    bar_count = sum(1 for t in valid_tokens if t == \"b-1\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"‚úì Valid tokens: {len(valid_tokens):,}\")\n",
    "        print(f\"‚úì Bars generated: {bar_count}\")\n",
    "    \n",
    "    # Rejoin\n",
    "    tokens = \" \".join(valid_tokens)\n",
    "    \n",
    "    # Optional: truncate at bar limit\n",
    "    if stop_at_bars is not None:\n",
    "        if verbose:\n",
    "            print(f\"‚úÇÔ∏è  Truncating to {stop_at_bars} bars...\")\n",
    "        \n",
    "        truncated = []\n",
    "        bars_seen = 0\n",
    "        \n",
    "        for token in valid_tokens:\n",
    "            truncated.append(token)\n",
    "            if token == \"b-1\":\n",
    "                bars_seen += 1\n",
    "                if bars_seen >= stop_at_bars:\n",
    "                    break\n",
    "        \n",
    "        tokens = \" \".join(truncated)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"‚úì Truncated to {len(truncated):,} tokens ({bars_seen} bars)\")\n",
    "    \n",
    "    # Add extra info to output dict\n",
    "    output_dict.update({\n",
    "        \"valid_tokens\": len(valid_tokens),\n",
    "        \"invalid_tokens\": invalid_count,\n",
    "        \"bars\": bar_count,\n",
    "        \"elapsed_time\": elapsed\n",
    "    })\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"‚ö° Speed: {len(valid_tokens)/elapsed:.1f} tokens/sec\")\n",
    "        print()\n",
    "    \n",
    "    return tokens, output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f2c19e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing generation...\n",
      "üéµ Starting generation...\n",
      "   Prompt: s-9 o-0 t-38 i-0 p-60 d-4 v-20 i-33 p-48 d-4 v-20 i-128 p-170 d-2 v-20\n",
      "   Max tokens: 200\n",
      "   Temperature: 0.7\n",
      "\n",
      "‚è≥ Generating tokens... Done! (4.1s)\n",
      "üìä Generated 200 raw tokens\n",
      "‚úì Valid tokens: 200\n",
      "‚úì Bars generated: 1\n",
      "‚ö° Speed: 49.3 tokens/sec\n",
      "\n",
      "Generated 200 tokens\n",
      "Speed: 45.67 tokens/sec\n",
      "\n",
      "First 100 chars: s-9 o-0 t-38 i-0 p-60 d-4 v-20 i-33 p-48 d-4 v-20 i-128 p-170 d-2 v-20 p-164 d-4 v-20 o-6 t-38 i-0 p\n"
     ]
    }
   ],
   "source": [
    "# Test generation\n",
    "print(\"Testing generation...\")\n",
    "test_tokens, test_info = generate_remigen_tokens(\n",
    "    model,\n",
    "    prompt=start_prompt,\n",
    "    temperature=temperature,\n",
    "    max_length=200,  # Short test\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(test_tokens.split())} tokens\")\n",
    "print(f\"Speed: {test_info['tokens_per_second']:.2f} tokens/sec\")\n",
    "print(f\"\\nFirst 100 chars: {test_tokens[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad73a32",
   "metadata": {},
   "source": [
    "## Decode Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7734474",
   "metadata": {},
   "source": [
    "REMIGEN ‚Üí MIDI Decoder\n",
    "\n",
    "Converts generated REMIGEN tokens to MIDI files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "62e12a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_remigen_to_midi(token_string, output_path):\n",
    "    \"\"\"\n",
    "    Decode REMIGEN tokens to MIDI file.\n",
    "    \n",
    "    Args:\n",
    "        token_string: Space-separated REMIGEN tokens\n",
    "        output_path: Path to save .mid file\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Split tokens\n",
    "        tokens = token_string.strip().split()\n",
    "        \n",
    "        # Initialize decoder\n",
    "        decoder = mp.MidiDecoder('REMIGEN')\n",
    "        \n",
    "        # Decode to MIDI object\n",
    "        midi_obj = decoder.decode_from_token_str_list(tokens)\n",
    "        \n",
    "        # Save\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        midi_obj.dump(output_path)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error decoding: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea7912b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Test decode successful: generated_music/test_decode.mid\n"
     ]
    }
   ],
   "source": [
    "# Test decoding\n",
    "test_output_path = output_dir / \"test_decode.mid\"\n",
    "success = decode_remigen_to_midi(test_tokens, str(test_output_path))\n",
    "\n",
    "if success:\n",
    "    print(f\"‚úì Test decode successful: {test_output_path}\")\n",
    "else:\n",
    "    print(\"‚úó Test decode failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e303708",
   "metadata": {},
   "source": [
    "## Generate Multiple Songs\n",
    "Generate a batch of music samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "464b915b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2 songs...\n",
      "\n",
      "Generating song 1/2...\n",
      "üéµ Starting generation...\n",
      "   Prompt: s-9 o-0 t-38 i-0 p-60 d-4 v-20 i-33 p-48 d-4 v-20 i-128 p-170 d-2 v-20\n",
      "   Max tokens: 8,000\n",
      "   Temperature: 0.7\n",
      "   Target bars: 32\n",
      "\n",
      "‚è≥ Generating tokens..."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerating song \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_songs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Generate tokens\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m tokens, info = \u001b[43mgenerate_remigen_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstop_at_bars\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Generate 32 bars per song\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Save info\u001b[39;00m\n\u001b[32m     18\u001b[39m song_data = {\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: i,\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m\"\u001b[39m: tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtokens_per_sec\u001b[39m\u001b[33m\"\u001b[39m: info[\u001b[33m\"\u001b[39m\u001b[33mtokens_per_second\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     24\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mgenerate_remigen_tokens\u001b[39m\u001b[34m(model, prompt, temperature, max_length, stop_at_bars, verbose)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m     38\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚è≥ Generating tokens...\u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m output_dict = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforbidden_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m[PAD]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m[EOS]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_structured_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     47\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m elapsed = time.time() - start_time\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch1/e20-fyp-xlstm-music-generation/e20fyptemp1/fyp-musicgen/repos/helibrunna/source/languagemodel.py:222\u001b[39m, in \u001b[36mLanguageModel.generate\u001b[39m\u001b[34m(self, prompt, temperature, max_length, end_tokens, forbidden_tokens, return_structured_output)\u001b[39m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    221\u001b[39m \u001b[38;5;66;03m# Generate the continuation.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m outputs.shape[\u001b[32m0\u001b[39m] == \u001b[32m1\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Mask the tokens.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/xlstm/xlstm_lm_model.py:52\u001b[39m, in \u001b[36mxLSTMLMModel.forward\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     50\u001b[39m x = \u001b[38;5;28mself\u001b[39m.token_embedding(idx)\n\u001b[32m     51\u001b[39m x = \u001b[38;5;28mself\u001b[39m.emb_dropout(x)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mxlstm_block_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.lm_head(x)\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/xlstm/xlstm_block_stack.py:120\u001b[39m, in \u001b[36mxLSTMBlockStack.forward\u001b[39m\u001b[34m(self, x, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor, **kwargs) -> torch.Tensor:\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         x = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.post_blocks_norm(x)\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/xlstm/blocks/xlstm_block.py:77\u001b[39m, in \u001b[36mxLSTMBlock.forward\u001b[39m\u001b[34m(self, x, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor, **kwargs) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mxlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mxlstm_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ffn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     79\u001b[39m         x = x + \u001b[38;5;28mself\u001b[39m.ffn(\u001b[38;5;28mself\u001b[39m.ffn_norm(x), **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/xlstm/blocks/slstm/layer.py:151\u001b[39m, in \u001b[36msLSTMLayer.forward\u001b[39m\u001b[34m(self, x, conv_state, slstm_state, return_last_state, **kwargs)\u001b[39m\n\u001b[32m    142\u001b[39m     x_conv = x\n\u001b[32m    144\u001b[39m i, f, z, o = (\n\u001b[32m    145\u001b[39m     \u001b[38;5;28mself\u001b[39m.fgate(x_conv),\n\u001b[32m    146\u001b[39m     \u001b[38;5;28mself\u001b[39m.igate(x_conv),\n\u001b[32m    147\u001b[39m     \u001b[38;5;28mself\u001b[39m.zgate(x),\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m.ogate(x),\n\u001b[32m    149\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m y, slstm_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mslstm_cell\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mslstm_state\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m y = \u001b[38;5;28mself\u001b[39m.dropout(y)\n\u001b[32m    157\u001b[39m out = \u001b[38;5;28mself\u001b[39m.group_norm(y).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).view(B, S, -\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/xlstm/blocks/slstm/cell.py:500\u001b[39m, in \u001b[36msLSTMCellBase.forward\u001b[39m\u001b[34m(self, input, state, lengths)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28mself\u001b[39m._permute_input(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m    499\u001b[39m states = \u001b[38;5;28mself\u001b[39m._get_state(\u001b[38;5;28minput\u001b[39m, state)\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m all_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m state = \u001b[38;5;28mself\u001b[39m._get_final_state(all_states)\n\u001b[32m    502\u001b[39m output = \u001b[38;5;28mself\u001b[39m._permute_output(all_states[\u001b[32m0\u001b[39m][\u001b[32m1\u001b[39m:])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/xlstm/blocks/slstm/cell.py:766\u001b[39m, in \u001b[36msLSTMCell_cuda._impl\u001b[39m\u001b[34m(self, training, input, state)\u001b[39m\n\u001b[32m    760\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_impl\u001b[39m(\n\u001b[32m    761\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    762\u001b[39m     training: \u001b[38;5;28mbool\u001b[39m,\n\u001b[32m    763\u001b[39m     \u001b[38;5;28minput\u001b[39m: torch.Tensor,\n\u001b[32m    764\u001b[39m     state: torch.Tensor,\n\u001b[32m    765\u001b[39m ) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recurrent_kernel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bias\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/autograd/function.py:574\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    572\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    573\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    576\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    577\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    578\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/torch/amp/autocast_mode.py:455\u001b[39m, in \u001b[36mcustom_fwd.<locals>.decorate_fwd\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cast_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    454\u001b[39m     args[\u001b[32m0\u001b[39m]._fwd_used_autocast = torch.is_autocast_enabled(device_type)\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    457\u001b[39m     autocast_context = torch.is_autocast_enabled(device_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/xlstm/lib/python3.11/site-packages/xlstm/blocks/slstm/cell.py:560\u001b[39m, in \u001b[36msLSTMCellFuncGenerator.<locals>.sLSTMCellFunction.forward\u001b[39m\u001b[34m(ctx, training, *inputs)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.enable_automatic_mixed_precision:\n\u001b[32m    554\u001b[39m     inputs = (\n\u001b[32m    555\u001b[39m         inputs[\u001b[32m0\u001b[39m].to(dtype=config.torch_dtype_w),\n\u001b[32m    556\u001b[39m         inputs[\u001b[32m1\u001b[39m].to(dtype=config.torch_dtype_s),\n\u001b[32m    557\u001b[39m         inputs[\u001b[32m2\u001b[39m].to(dtype=config.torch_dtype_r),\n\u001b[32m    558\u001b[39m         inputs[\u001b[32m3\u001b[39m].to(dtype=config.torch_dtype_b),\n\u001b[32m    559\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m states, cache_g_r, cache_g_i = \u001b[43mslstm_mod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    562\u001b[39m ctx.save_for_backward(*inputs[\u001b[32m2\u001b[39m:], states, cache_g_r, cache_g_i)\n\u001b[32m    563\u001b[39m ctx.training = training\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(f\"Generating {num_songs} songs...\\n\")\n",
    "\n",
    "generated_songs = []\n",
    "\n",
    "for i in range(num_songs):\n",
    "    print(f\"Generating song {i+1}/{num_songs}...\")\n",
    "    \n",
    "    # Generate tokens\n",
    "    tokens, info = generate_remigen_tokens(\n",
    "        model,\n",
    "        prompt=start_prompt,\n",
    "        temperature=temperature,\n",
    "        max_length=max_length,\n",
    "        stop_at_bars=32  # Generate 32 bars per song\n",
    "    )\n",
    "    \n",
    "    # Save info\n",
    "    song_data = {\n",
    "        \"id\": i,\n",
    "        \"tokens\": tokens,\n",
    "        \"num_tokens\": len(tokens.split()),\n",
    "        \"generation_time\": info[\"elapsed_time\"],\n",
    "        \"tokens_per_sec\": info[\"tokens_per_second\"]\n",
    "    }\n",
    "    generated_songs.append(song_data)\n",
    "    \n",
    "    print(f\"  Generated {song_data['num_tokens']} tokens in {song_data['generation_time']:.2f}s\")\n",
    "    print(f\"  Speed: {song_data['tokens_per_sec']:.2f} tokens/sec\\n\")\n",
    "\n",
    "print(f\"‚úì Generated {len(generated_songs)} songs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78911664",
   "metadata": {},
   "source": [
    "### Decode All Generated Songs to MIDI\n",
    "\n",
    "Convert all generated token sequences to MIDI files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54597ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding to MIDI files...\n",
      "\n",
      "‚úì Song 0: generated_song_000.mid\n",
      "‚úì Song 1: generated_song_001.mid\n",
      "\n",
      "‚úì Successfully decoded: 2/2\n"
     ]
    }
   ],
   "source": [
    "print(\"Decoding to MIDI files...\\n\")\n",
    "\n",
    "successful = 0\n",
    "failed = 0\n",
    "\n",
    "for song_data in generated_songs:\n",
    "    song_id = song_data[\"id\"]\n",
    "    tokens = song_data[\"tokens\"]\n",
    "    \n",
    "    # Output path\n",
    "    midi_path = output_dir / f\"generated_song_{song_id:03d}.mid\"\n",
    "    \n",
    "    # Decode\n",
    "    success = decode_remigen_to_midi(tokens, str(midi_path))\n",
    "    \n",
    "    if success:\n",
    "        successful += 1\n",
    "        print(f\"‚úì Song {song_id}: {midi_path.name}\")\n",
    "    else:\n",
    "        failed += 1\n",
    "        print(f\"‚úó Song {song_id}: Failed to decode\")\n",
    "\n",
    "print(f\"\\n‚úì Successfully decoded: {successful}/{len(generated_songs)}\")\n",
    "if failed > 0:\n",
    "    print(f\"‚úó Failed: {failed}/{len(generated_songs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60502a7b",
   "metadata": {},
   "source": [
    "## Analyze Generated MIDI Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "255fbe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing generated music...\n",
      "\n",
      "Song 0:\n",
      "  Duration: 24.64s\n",
      "  Instruments: 4\n",
      "  Total notes: 412\n",
      "    0: 42 notes (Program 0)\n",
      "    2: 103 notes (Program 2)\n",
      "    33: 188 notes (Program 33)\n",
      "    128: 79 notes (Drums)\n",
      "\n",
      "Song 1:\n",
      "  Duration: 19.84s\n",
      "  Instruments: 6\n",
      "  Total notes: 509\n",
      "    0: 270 notes (Program 0)\n",
      "    6: 3 notes (Program 6)\n",
      "    25: 59 notes (Program 25)\n",
      "    33: 24 notes (Program 33)\n",
      "    40: 27 notes (Program 40)\n",
      "    128: 126 notes (Drums)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pretty_midi\n",
    "\n",
    "print(\"Analyzing generated music...\\n\")\n",
    "\n",
    "for song_data in generated_songs[:3]:  # Analyze first 3 songs\n",
    "    song_id = song_data[\"id\"]\n",
    "    midi_path = output_dir / f\"generated_song_{song_id:03d}.mid\"\n",
    "    \n",
    "    if not midi_path.exists():\n",
    "        continue\n",
    "    \n",
    "    # Load MIDI\n",
    "    midi = pretty_midi.PrettyMIDI(str(midi_path))\n",
    "    \n",
    "    print(f\"Song {song_id}:\")\n",
    "    print(f\"  Duration: {midi.get_end_time():.2f}s\")\n",
    "    print(f\"  Instruments: {len(midi.instruments)}\")\n",
    "    print(f\"  Total notes: {sum(len(inst.notes) for inst in midi.instruments)}\")\n",
    "    \n",
    "    # Show instruments\n",
    "    for inst in midi.instruments:\n",
    "        inst_type = \"Drums\" if inst.is_drum else f\"Program {inst.program}\"\n",
    "        print(f\"    {inst.name}: {len(inst.notes)} notes ({inst_type})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5600537",
   "metadata": {},
   "source": [
    "## Save Generated Tokens\n",
    "\n",
    "Save raw token sequences for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ab816eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved token files to generated_music/tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokens_dir = output_dir / \"tokens\"\n",
    "tokens_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for song_data in generated_songs:\n",
    "    song_id = song_data[\"id\"]\n",
    "    tokens = song_data[\"tokens\"]\n",
    "    \n",
    "    token_path = tokens_dir / f\"generated_song_{song_id:03d}.txt\"\n",
    "    \n",
    "    with open(token_path, 'w') as f:\n",
    "        f.write(tokens)\n",
    "\n",
    "print(f\"‚úì Saved token files to {tokens_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f64b36",
   "metadata": {},
   "source": [
    "## Generation Summary\n",
    "\n",
    "Summary of the music generation session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "133f62c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MUSIC GENERATION SUMMARY\n",
      "============================================================\n",
      "Model: /scratch1/e20-fyp-xlstm-music-generation/e20fyptemp1/fyp-musicgen/repos/helibrunna/output/lmd_remigen_xlstm/run_20260106-1858\n",
      "Songs generated: 2\n",
      "Temperature: 0.8\n",
      "Max tokens: 2048\n",
      "Output directory: generated_music\n",
      "\n",
      "Generated files:\n",
      "  MIDI files: generated_music/*.mid\n",
      "  Token files: generated_music/tokens/*.txt\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MUSIC GENERATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {model_path}\")\n",
    "print(f\"Songs generated: {len(generated_songs)}\")\n",
    "print(f\"Temperature: {temperature}\")\n",
    "print(f\"Max tokens: {max_length}\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"\\nGenerated files:\")\n",
    "print(f\"  MIDI files: {output_dir}/*.mid\")\n",
    "print(f\"  Token files: {tokens_dir}/*.txt\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325d5185",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xlstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
